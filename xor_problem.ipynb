{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import neural_network_library as nnl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Created XOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the xor dataset\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "], dtype = np.float32)\n",
    "\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype= np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Sigmoid and Tanh Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_sigmoid = nnl.Sequential(\n",
    "    layers=[\n",
    "        nnl.Linear(input_size = 2, output_size=2),\n",
    "        nnl.Sigmoid(),\n",
    "        nnl.Linear(input_size = 2, output_size = 1),\n",
    "        nnl.Sigmoid()\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_tanh = nnl.Sequential(\n",
    "    layers=[\n",
    "        nnl.Linear(input_size = 2, output_size=2),\n",
    "        nnl.Tanh(),\n",
    "        nnl.Linear(input_size = 2, output_size = 1),\n",
    "        nnl.Sigmoid()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "loss_sig_fn = nnl.BceLoss()\n",
    "loss_tanh_fn = nnl.BceLoss()\n",
    "learning_rate_sigmoid = 1\n",
    "learning_rate_tanh = 0.5\n",
    "n_epochs = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sigmoid and Tanh Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Sigmoid loss: 0.008210874906870648\n",
      "Epoch 1000, Sigmoid loss: 0.008154491371567426\n",
      "Epoch 2000, Sigmoid loss: 0.0080992344765236\n",
      "Epoch 3000, Sigmoid loss: 0.00804506741273565\n",
      "Epoch 4000, Sigmoid loss: 0.007991955027828394\n",
      "Epoch 5000, Sigmoid loss: 0.007939863731605452\n",
      "Epoch 6000, Sigmoid loss: 0.007888761408089633\n",
      "Epoch 7000, Sigmoid loss: 0.0078386173335329\n",
      "Epoch 8000, Sigmoid loss: 0.007789402099922982\n",
      "Epoch 9000, Sigmoid loss: 0.007741087543558867\n",
      "Epoch 10000, Sigmoid loss: 0.007693646678300843\n",
      "Epoch 11000, Sigmoid loss: 0.0076470536331398\n",
      "Epoch 12000, Sigmoid loss: 0.007601283593759364\n",
      "Epoch 13000, Sigmoid loss: 0.007556312747792285\n",
      "Epoch 14000, Sigmoid loss: 0.007512118233499147\n",
      "Epoch 15000, Sigmoid loss: 0.00746867809161979\n",
      "Epoch 16000, Sigmoid loss: 0.00742597122016713\n",
      "Epoch 17000, Sigmoid loss: 0.007383977331955161\n",
      "Epoch 18000, Sigmoid loss: 0.007342676914667201\n",
      "Epoch 19000, Sigmoid loss: 0.007302051193286738\n",
      "Epoch 20000, Sigmoid loss: 0.007262082094727958\n",
      "Epoch 21000, Sigmoid loss: 0.007222752214515242\n",
      "Epoch 22000, Sigmoid loss: 0.007184044785373552\n",
      "Epoch 23000, Sigmoid loss: 0.0071459436476004915\n",
      "Epoch 24000, Sigmoid loss: 0.007108433221103662\n",
      "Epoch 25000, Sigmoid loss: 0.007071498478991982\n",
      "Epoch 26000, Sigmoid loss: 0.007035124922621629\n",
      "Epoch 27000, Sigmoid loss: 0.006999298558001707\n",
      "Epoch 28000, Sigmoid loss: 0.006964005873474597\n",
      "Epoch 29000, Sigmoid loss: 0.006929233818588222\n",
      "Epoch 30000, Sigmoid loss: 0.006894969784088396\n",
      "Epoch 31000, Sigmoid loss: 0.006861201582959039\n",
      "Epoch 32000, Sigmoid loss: 0.00682791743244902\n",
      "Epoch 33000, Sigmoid loss: 0.006795105937023093\n",
      "Epoch 34000, Sigmoid loss: 0.0067627560721827355\n",
      "Epoch 35000, Sigmoid loss: 0.0067308571691044785\n",
      "Epoch 36000, Sigmoid loss: 0.006699398900047594\n",
      "Epoch 37000, Sigmoid loss: 0.006668371264485862\n",
      "Epoch 38000, Sigmoid loss: 0.006637764575921236\n",
      "Epoch 39000, Sigmoid loss: 0.006607569449341451\n",
      "Epoch 40000, Sigmoid loss: 0.006577776789282075\n",
      "Epoch 41000, Sigmoid loss: 0.006548377778462136\n",
      "Epoch 42000, Sigmoid loss: 0.006519363866958664\n",
      "Epoch 43000, Sigmoid loss: 0.006490726761891125\n",
      "Epoch 44000, Sigmoid loss: 0.00646245841758789\n",
      "Epoch 45000, Sigmoid loss: 0.006434551026207864\n",
      "Epoch 46000, Sigmoid loss: 0.006406997008792463\n",
      "Epoch 47000, Sigmoid loss: 0.006379789006726177\n",
      "Epoch 48000, Sigmoid loss: 0.006352919873581544\n",
      "Epoch 49000, Sigmoid loss: 0.00632638266733055\n",
      "Epoch 50000, Sigmoid loss: 0.00630017064290167\n",
      "Epoch 51000, Sigmoid loss: 0.006274277245065218\n",
      "Epoch 52000, Sigmoid loss: 0.006248696101630593\n",
      "Epoch 53000, Sigmoid loss: 0.006223421016937005\n",
      "Epoch 54000, Sigmoid loss: 0.006198445965626966\n",
      "Epoch 55000, Sigmoid loss: 0.006173765086683811\n",
      "Epoch 56000, Sigmoid loss: 0.006149372677722783\n",
      "Epoch 57000, Sigmoid loss: 0.00612526318952288\n",
      "Epoch 58000, Sigmoid loss: 0.00610143122078631\n",
      "Epoch 59000, Sigmoid loss: 0.006077871513115323\n",
      "Epoch 60000, Sigmoid loss: 0.0060545789461961845\n",
      "Epoch 61000, Sigmoid loss: 0.006031548533179238\n",
      "Epoch 62000, Sigmoid loss: 0.006008775416246784\n",
      "Epoch 63000, Sigmoid loss: 0.005986254862358651\n",
      "Epoch 64000, Sigmoid loss: 0.005963982259168838\n",
      "Epoch 65000, Sigmoid loss: 0.00594195311110283\n",
      "Epoch 66000, Sigmoid loss: 0.005920163035590327\n",
      "Epoch 67000, Sigmoid loss: 0.005898607759445024\n",
      "Epoch 68000, Sigmoid loss: 0.0058772831153852114\n",
      "Epoch 69000, Sigmoid loss: 0.005856185038687948\n",
      "Epoch 70000, Sigmoid loss: 0.0058353095639719875\n",
      "Epoch 71000, Sigmoid loss: 0.005814652822102857\n",
      "Epoch 72000, Sigmoid loss: 0.0057942110372145444\n",
      "Epoch 73000, Sigmoid loss: 0.005773980523843444\n",
      "Epoch 74000, Sigmoid loss: 0.005753957684168796\n",
      "Epoch 75000, Sigmoid loss: 0.005734139005355318\n",
      "Epoch 76000, Sigmoid loss: 0.005714521056993355\n",
      "Epoch 77000, Sigmoid loss: 0.005695100488632829\n",
      "Epoch 78000, Sigmoid loss: 0.005675874027406873\n",
      "Epoch 79000, Sigmoid loss: 0.0056568384757403\n",
      "Epoch 80000, Sigmoid loss: 0.005637990709141058\n",
      "Epoch 81000, Sigmoid loss: 0.005619327674069605\n",
      "Epoch 82000, Sigmoid loss: 0.0056008463858840465\n",
      "Epoch 83000, Sigmoid loss: 0.005582543926857432\n",
      "Epoch 84000, Sigmoid loss: 0.0055644174442640165\n",
      "Epoch 85000, Sigmoid loss: 0.005546464148532068\n",
      "Epoch 86000, Sigmoid loss: 0.005528681311460474\n",
      "Epoch 87000, Sigmoid loss: 0.005511066264495998\n",
      "Epoch 88000, Sigmoid loss: 0.005493616397070218\n",
      "Epoch 89000, Sigmoid loss: 0.005476329154991434\n",
      "Epoch 90000, Sigmoid loss: 0.005459202038891345\n",
      "Epoch 91000, Sigmoid loss: 0.005442232602724139\n",
      "Epoch 92000, Sigmoid loss: 0.005425418452314405\n",
      "Epoch 93000, Sigmoid loss: 0.005408757243953809\n",
      "Epoch 94000, Sigmoid loss: 0.005392246683043556\n",
      "Epoch 95000, Sigmoid loss: 0.005375884522781161\n",
      "Epoch 96000, Sigmoid loss: 0.005359668562889894\n",
      "Epoch 97000, Sigmoid loss: 0.005343596648389603\n",
      "Epoch 98000, Sigmoid loss: 0.005327666668405997\n",
      "Epoch 99000, Sigmoid loss: 0.005311876555018629\n"
     ]
    }
   ],
   "source": [
    "# Train the model_sigmoid\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred_sig = model_sigmoid.forward(X)\n",
    "    loss_sig = loss_sig_fn.forward(y_pred_sig, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_sig = loss_sig_fn.backward()\n",
    "    model_sigmoid.backward(grad_sig)\n",
    "\n",
    "    # Update the weights\n",
    "    for layer in model_sigmoid.layers:\n",
    "        if isinstance(layer, nnl.Linear):\n",
    "            layer.weights -= learning_rate_sigmoid * layer.grad_weights\n",
    "            layer.bias -= learning_rate_sigmoid * layer.grad_bias\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Sigmoid loss: {loss_sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Tanh loss: 0.005703635132524896\n",
      "Epoch 1000, Tanh loss: 0.005673685660181101\n",
      "Epoch 2000, Tanh loss: 0.0056441981965530625\n",
      "Epoch 3000, Tanh loss: 0.005615161027623237\n",
      "Epoch 4000, Tanh loss: 0.005586562850175891\n",
      "Epoch 5000, Tanh loss: 0.005558392753482828\n",
      "Epoch 6000, Tanh loss: 0.005530640201974483\n",
      "Epoch 7000, Tanh loss: 0.0055032950188380215\n",
      "Epoch 8000, Tanh loss: 0.0054763473704815945\n",
      "Epoch 9000, Tanh loss: 0.0054497877518124935\n",
      "Epoch 10000, Tanh loss: 0.0054236069722792815\n",
      "Epoch 11000, Tanh loss: 0.005397796142631145\n",
      "Epoch 12000, Tanh loss: 0.005372346662351819\n",
      "Epoch 13000, Tanh loss: 0.0053472502077272785\n",
      "Epoch 14000, Tanh loss: 0.0053224987205107745\n",
      "Epoch 15000, Tanh loss: 0.005298084397148144\n",
      "Epoch 16000, Tanh loss: 0.005273999678534226\n",
      "Epoch 17000, Tanh loss: 0.005250237240266549\n",
      "Epoch 18000, Tanh loss: 0.005226789983369768\n",
      "Epoch 19000, Tanh loss: 0.005203651025464185\n",
      "Epoch 20000, Tanh loss: 0.005180813692352786\n",
      "Epoch 21000, Tanh loss: 0.0051582715100036754\n",
      "Epoch 22000, Tanh loss: 0.0051360181969070575\n",
      "Epoch 23000, Tanh loss: 0.005114047656784879\n",
      "Epoch 24000, Tanh loss: 0.005092353971635472\n",
      "Epoch 25000, Tanh loss: 0.005070931395093918\n",
      "Epoch 26000, Tanh loss: 0.005049774346091908\n",
      "Epoch 27000, Tanh loss: 0.005028877402802128\n",
      "Epoch 28000, Tanh loss: 0.005008235296849959\n",
      "Epoch 29000, Tanh loss: 0.004987842907781622\n",
      "Epoch 30000, Tanh loss: 0.0049676952577724556\n",
      "Epoch 31000, Tanh loss: 0.004947787506565736\n",
      "Epoch 32000, Tanh loss: 0.004928114946628088\n",
      "Epoch 33000, Tanh loss: 0.00490867299851274\n",
      "Epoch 34000, Tanh loss: 0.004889457206417951\n",
      "Epoch 35000, Tanh loss: 0.004870463233932669\n",
      "Epoch 36000, Tanh loss: 0.004851686859959852\n",
      "Epoch 37000, Tanh loss: 0.004833123974807821\n",
      "Epoch 38000, Tanh loss: 0.004814770576442804\n",
      "Epoch 39000, Tanh loss: 0.004796622766893685\n",
      "Epoch 40000, Tanh loss: 0.004778676748803218\n",
      "Epoch 41000, Tanh loss: 0.004760928822116486\n",
      "Epoch 42000, Tanh loss: 0.004743375380902737\n",
      "Epoch 43000, Tanh loss: 0.004726012910302653\n",
      "Epoch 44000, Tanh loss: 0.004708837983595268\n",
      "Epoch 45000, Tanh loss: 0.004691847259380031\n",
      "Epoch 46000, Tanh loss: 0.004675037478867541\n",
      "Epoch 47000, Tanh loss: 0.004658405463275178\n",
      "Epoch 48000, Tanh loss: 0.0046419481113219065\n",
      "Epoch 49000, Tanh loss: 0.004625662396818518\n",
      "Epoch 50000, Tanh loss: 0.004609545366348384\n",
      "Epoch 51000, Tanh loss: 0.004593594137035505\n",
      "Epoch 52000, Tanh loss: 0.004577805894395\n",
      "Epoch 53000, Tanh loss: 0.004562177890263825\n",
      "Epoch 54000, Tanh loss: 0.004546707440807116\n",
      "Epoch 55000, Tanh loss: 0.004531391924596641\n",
      "Epoch 56000, Tanh loss: 0.004516228780759902\n",
      "Epoch 57000, Tanh loss: 0.004501215507195209\n",
      "Epoch 58000, Tanh loss: 0.004486349658850696\n",
      "Epoch 59000, Tanh loss: 0.004471628846064497\n",
      "Epoch 60000, Tanh loss: 0.004457050732963542\n",
      "Epoch 61000, Tanh loss: 0.004442613035919018\n",
      "Epoch 62000, Tanh loss: 0.004428313522054252\n",
      "Epoch 63000, Tanh loss: 0.004414150007805916\n",
      "Epoch 64000, Tanh loss: 0.0044001203575337175\n",
      "Epoch 65000, Tanh loss: 0.004386222482178402\n",
      "Epoch 66000, Tanh loss: 0.004372454337964225\n",
      "Epoch 67000, Tanh loss: 0.004358813925146703\n",
      "Epoch 68000, Tanh loss: 0.004345299286801514\n",
      "Epoch 69000, Tanh loss: 0.004331908507653923\n",
      "Epoch 70000, Tanh loss: 0.0043186397129471405\n",
      "Epoch 71000, Tanh loss: 0.004305491067348266\n",
      "Epoch 72000, Tanh loss: 0.004292460773889767\n",
      "Epoch 73000, Tanh loss: 0.004279547072945365\n",
      "Epoch 74000, Tanh loss: 0.004266748241239534\n",
      "Epoch 75000, Tanh loss: 0.004254062590888601\n",
      "Epoch 76000, Tanh loss: 0.00424148846847266\n",
      "Epoch 77000, Tanh loss: 0.004229024254137013\n",
      "Epoch 78000, Tanh loss: 0.0042166683607220284\n",
      "Epoch 79000, Tanh loss: 0.00420441923292038\n",
      "Epoch 80000, Tanh loss: 0.004192275346460602\n",
      "Epoch 81000, Tanh loss: 0.004180235207316596\n",
      "Epoch 82000, Tanh loss: 0.004168297350940628\n",
      "Epoch 83000, Tanh loss: 0.004156460341520805\n",
      "Epoch 84000, Tanh loss: 0.0041447227712607905\n",
      "Epoch 85000, Tanh loss: 0.004133083259681529\n",
      "Epoch 86000, Tanh loss: 0.004121540452944586\n",
      "Epoch 87000, Tanh loss: 0.004110093023194879\n",
      "Epoch 88000, Tanh loss: 0.00409873966792399\n",
      "Epoch 89000, Tanh loss: 0.00408747910935199\n",
      "Epoch 90000, Tanh loss: 0.004076310093827848\n",
      "Epoch 91000, Tanh loss: 0.004065231391246975\n",
      "Epoch 92000, Tanh loss: 0.0040542417944867135\n",
      "Epoch 93000, Tanh loss: 0.00404334011885753\n",
      "Epoch 94000, Tanh loss: 0.004032525201570135\n",
      "Epoch 95000, Tanh loss: 0.0040217959012185805\n",
      "Epoch 96000, Tanh loss: 0.004011151097277749\n",
      "Epoch 97000, Tanh loss: 0.004000589689615055\n",
      "Epoch 98000, Tanh loss: 0.003990110598016452\n",
      "Epoch 99000, Tanh loss: 0.003979712761725527\n"
     ]
    }
   ],
   "source": [
    "# Train the model_tanh\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred_tanh = model_tanh.forward(X)\n",
    "    loss_tanh = loss_tanh_fn.forward(y_pred_tanh, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_tanh = loss_tanh_fn.backward()\n",
    "    model_tanh.backward(grad_tanh)\n",
    "\n",
    "    # Update the weights\n",
    "    for layer in model_tanh.layers:\n",
    "        if isinstance(layer, nnl.Linear):\n",
    "            layer.weights -= learning_rate_tanh * layer.grad_weights\n",
    "            layer.bias -= learning_rate_tanh * layer.grad_bias\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Tanh loss: {loss_tanh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions after training for sigmoid:\n",
      "[[0.005]\n",
      " [0.995]\n",
      " [0.995]\n",
      " [0.006]]\n",
      "weights sigmoid: [[-6.92366536 -6.92071224]\n",
      " [-5.56788894 -5.56722558]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredictions after training for sigmoid:\")\n",
    "print(np.round(model_sigmoid.forward(X), 3))\n",
    "model_sigmoid.save(r\"Models\\xor_model_sigmoid.npz\")\n",
    "print('weights sigmoid:', model_sigmoid.layers[0].weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions after training for tanh:\n",
      "[[0.005]\n",
      " [0.997]\n",
      " [0.997]\n",
      " [0.005]]\n",
      "weights tanh: [[-3.81333813 -3.80589103]\n",
      " [ 2.96233241  2.9606089 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredictions after training for tanh:\")\n",
    "print(np.round(model_tanh.forward(X), 3))\n",
    "model_tanh.save(r\"Models\\xor_model_tanh.npz\")\n",
    "print('weights tanh:', model_tanh.layers[0].weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground truth:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGround truth:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
