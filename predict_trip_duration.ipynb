{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import neural_network_library as nnl\n",
    "\n",
    "dataset = np.load('nyc_taxi_data.npy', allow_pickle=True).item()\n",
    "X_train, y_train, X_test, y_test = dataset['X_train'], dataset['y_train'], dataset['X_test'], dataset['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Data and Converting to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n",
      "       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n",
      "       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
      "879655   id2425795          1  2016-01-08 23:55:11  2016-01-09 00:04:32   \n",
      "646838   id0767831          2  2016-03-05 09:52:06  2016-03-05 10:00:12   \n",
      "1138713  id0449104          1  2016-04-09 16:03:53  2016-04-09 16:21:22   \n",
      "864716   id3030157          1  2016-01-06 11:12:44  2016-01-06 11:19:49   \n",
      "434927   id1584885          1  2016-06-26 09:10:56  2016-06-26 09:17:44   \n",
      "\n",
      "         passenger_count  pickup_longitude  pickup_latitude  \\\n",
      "879655                 1        -73.955551        40.773346   \n",
      "646838                 1        -73.962181        40.763599   \n",
      "1138713                1        -73.977486        40.751842   \n",
      "864716                 1        -73.970001        40.762363   \n",
      "434927                 1        -73.950348        40.771561   \n",
      "\n",
      "         dropoff_longitude  dropoff_latitude store_and_fwd_flag  \n",
      "879655          -73.973640         40.763500                  N  \n",
      "646838          -73.980377         40.764919                  N  \n",
      "1138713         -74.011688         40.718925                  N  \n",
      "864716          -73.963264         40.774666                  N  \n",
      "434927          -73.968178         40.762409                  N  \n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          vendor_id  passenger_count  pickup_longitude  pickup_latitude  \\\n",
      "count  1.312779e+06     1.312779e+06      1.312779e+06     1.312779e+06   \n",
      "mean   1.534878e+00     1.664126e+00     -7.397350e+01     4.075093e+01   \n",
      "std    4.987823e-01     1.313950e+00      7.351224e-02     3.291198e-02   \n",
      "min    1.000000e+00     0.000000e+00     -1.219333e+02     3.435970e+01   \n",
      "25%    1.000000e+00     1.000000e+00     -7.399187e+01     4.073735e+01   \n",
      "50%    2.000000e+00     1.000000e+00     -7.398174e+01     4.075410e+01   \n",
      "75%    2.000000e+00     2.000000e+00     -7.396734e+01     4.076835e+01   \n",
      "max    2.000000e+00     9.000000e+00     -6.133553e+01     5.188108e+01   \n",
      "\n",
      "       dropoff_longitude  dropoff_latitude  \n",
      "count       1.312779e+06      1.312779e+06  \n",
      "mean       -7.397342e+01      4.075181e+01  \n",
      "std         7.316118e-02      3.579324e-02  \n",
      "min        -1.219333e+02      3.218114e+01  \n",
      "25%        -7.399133e+01      4.073589e+01  \n",
      "50%        -7.397975e+01      4.075453e+01  \n",
      "75%        -7.396302e+01      4.076980e+01  \n",
      "max        -6.133553e+01      4.392103e+01  \n"
     ]
    }
   ],
   "source": [
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1312779 entries, 879655 to 121958\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   id                  1312779 non-null  object \n",
      " 1   vendor_id           1312779 non-null  int64  \n",
      " 2   pickup_datetime     1312779 non-null  object \n",
      " 3   dropoff_datetime    1312779 non-null  object \n",
      " 4   passenger_count     1312779 non-null  int64  \n",
      " 5   pickup_longitude    1312779 non-null  float64\n",
      " 6   pickup_latitude     1312779 non-null  float64\n",
      " 7   dropoff_longitude   1312779 non-null  float64\n",
      " 8   dropoff_latitude    1312779 non-null  float64\n",
      " 9   store_and_fwd_flag  1312779 non-null  object \n",
      "dtypes: float64(4), int64(2), object(4)\n",
      "memory usage: 110.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training feature:\n",
      " id                    0\n",
      "vendor_id             0\n",
      "pickup_datetime       0\n",
      "dropoff_datetime      0\n",
      "passenger_count       0\n",
      "pickup_longitude      0\n",
      "pickup_latitude       0\n",
      "dropoff_longitude     0\n",
      "dropoff_latitude      0\n",
      "store_and_fwd_flag    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Missing values in training feature:\\n', X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c  # Distance in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['distance_km'] = haversine(X_train['pickup_latitude'], X_train['pickup_longitude'], X_train['dropoff_latitude'], X_train['dropoff_longitude'])\n",
    "X_test['distance_km'] = haversine(X_test['pickup_latitude'], X_test['pickup_longitude'], X_test['dropoff_latitude'], X_test['dropoff_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['pickup_datetime'] = pd.to_datetime(X_train['pickup_datetime'])\n",
    "X_train['pickup_hour'] = X_train['pickup_datetime'].dt.hour\n",
    "X_train['pickup_dow'] = X_train['pickup_datetime'].dt.dayofweek\n",
    "X_train['pickup_month'] = X_train['pickup_datetime'].dt.month\n",
    "\n",
    "\n",
    "X_train[\"pickup_hour_sin\"] = np.sin(2 * np.pi * X_train[\"pickup_hour\"] / 24)\n",
    "X_train[\"pickup_hour_cos\"] = np.cos(2 * np.pi * X_train[\"pickup_hour\"] / 24)\n",
    "X_train[\"pickup_dow_sin\"] = np.sin(2 * np.pi * X_train[\"pickup_dow\"] / 7)\n",
    "X_train[\"pickup_dow_cos\"] = np.cos(2 * np.pi * X_train[\"pickup_dow\"] / 7)\n",
    "X_train['is_weekend'] = X_train['pickup_dow'].isin([5, 6]).astype(int)\n",
    "\n",
    "X_test['pickup_datetime'] = pd.to_datetime(X_test['pickup_datetime'])\n",
    "X_test['pickup_hour'] = X_test['pickup_datetime'].dt.hour\n",
    "X_test['pickup_dow'] = X_test['pickup_datetime'].dt.dayofweek\n",
    "X_test['pickup_month'] = X_test['pickup_datetime'].dt.month\n",
    "\n",
    "X_test[\"pickup_hour_sin\"] = np.sin(2 * np.pi * X_test[\"pickup_hour\"] / 24)\n",
    "X_test[\"pickup_hour_cos\"] = np.cos(2 * np.pi * X_test[\"pickup_hour\"] / 24)\n",
    "X_test[\"pickup_dow_sin\"] = np.sin(2 * np.pi * X_test[\"pickup_dow\"] / 7)\n",
    "X_test[\"pickup_dow_cos\"] = np.cos(2 * np.pi * X_test[\"pickup_dow\"] / 7)\n",
    "X_test['is_weekend'] = X_test['pickup_dow'].isin([5, 6]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['id', 'dropoff_datetime', 'pickup_datetime'], axis=1)\n",
    "X_test = X_test.drop(['id', 'dropoff_datetime', 'pickup_datetime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879655     1\n",
      "646838     2\n",
      "1138713    1\n",
      "864716     1\n",
      "434927     1\n",
      "          ..\n",
      "259178     2\n",
      "1414414    1\n",
      "131932     2\n",
      "671155     1\n",
      "121958     2\n",
      "Name: vendor_id, Length: 1312779, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train['vendor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"vendor_id\"] = X_train[\"vendor_id\"].astype(int)\n",
    "X_train[\"store_and_fwd_flag\"] = (X_train[\"store_and_fwd_flag\"] == \"Y\").astype(int)\n",
    "\n",
    "X_test[\"vendor_id\"] = X_test[\"vendor_id\"].astype(int)\n",
    "X_test[\"store_and_fwd_flag\"] = (X_test[\"store_and_fwd_flag\"] == \"Y\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_features = ['passenger_count', 'distance_km', 'pickup_month']\n",
    "\n",
    "X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "\n",
    "X_test[num_features] = scaler.transform(X_test[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = [\n",
    "    \"vendor_id\",\n",
    "    \"passenger_count\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"distance_km\",\n",
    "    \"pickup_hour_sin\",\n",
    "    \"pickup_hour_cos\",\n",
    "    \"pickup_dow_sin\",\n",
    "    \"pickup_dow_cos\",\n",
    "    \"is_weekend\",\n",
    "    \"pickup_month\",\n",
    "    \"pickup_latitude\",\n",
    "    \"pickup_longitude\",\n",
    "]\n",
    "\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1050223, 12)\n",
      "Validation set shape: (262556, 12)\n",
      "Test set shape: (145865, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_final, y_train_log, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train_split.shape)\n",
    "print(\"Validation set shape:\", X_val_split.shape)\n",
    "print(\"Test set shape:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21164\\31432080.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mtrain_loss_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0my_val_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_val_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0my_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ed5ch\\anaconda3\\envs\\ml-environment\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "model = nnl.Sequential(\n",
    "    layers = [\n",
    "    nnl.Linear(12, 64),\n",
    "    nnl.ReLU(),\n",
    "    nnl.Linear(64, 64),\n",
    "    nnl.ReLU(),\n",
    "    nnl.Linear(64, 1),\n",
    "    nnl.Sigmoid()]\n",
    ")\n",
    "\n",
    "loss = nnl.MseLoss()\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "indices = np.arange(X_train_split.shape[0])\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "patience = 3\n",
    "\n",
    "num_samples = X_train_split.shape[0]\n",
    "indices = np.arange(num_samples)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle training data each epoch\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Mini-batch loop\n",
    "    batch_losses = []\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_idx = indices[start:end]\n",
    "        X_batch = X_train_split.iloc[batch_idx]\n",
    "        y_batch = y_train_split.iloc[batch_idx]\n",
    "\n",
    "        y_pred = model.forward(X_batch)\n",
    "        loss_value = loss.forward(y_pred, y_batch)\n",
    "        batch_losses.append(loss_value)\n",
    "\n",
    "        grad_loss = loss.backward()\n",
    "        model.backward(grad_loss)\n",
    "\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, nnl.Linear):\n",
    "                layer.weights -= learning_rate * layer.grad_weights\n",
    "                layer.bias -= learning_rate * layer.grad_bias\n",
    "\n",
    "    train_loss_epoch = np.mean(batch_losses)\n",
    "    train_losses.append(train_loss_epoch)\n",
    "\n",
    "    y_val_split = y_val_split\n",
    "    y_val_pred = model.forward(X_val_split)\n",
    "    val_loss = loss.forward(y_val_pred, y_val_split)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'epcoh: {epoch}, train_loss: {train_loss_epoch}, val_loss: {val_loss}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training finished')\n",
    "print('Best validation loss:', best_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('losses graph')\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
